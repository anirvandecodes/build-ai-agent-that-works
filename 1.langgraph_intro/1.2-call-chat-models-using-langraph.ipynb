{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d604ac2-8633-4e75-bcaa-c907ebf0d6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LangGraph with Databricks LLM - Code Explanation\n",
    "\n",
    "This code demonstrates a simple conversational AI workflow using **LangGraph** (a graph-based framework for building language model applications) integrated with **Databricks' LLM serving endpoint**.\n",
    "\n",
    "\n",
    "## ü§ñ **Core Function: `call_llm`**\n",
    "\n",
    "```python\n",
    "def call_llm(state: MessagesState):\n",
    "    system_msg = {\"role\": \"system\", \"content\": \"Reply only with plain text. No formatting.\"}\n",
    "    all_msgs = [system_msg] + state[\"messages\"]\n",
    "    return {\"messages\": [llm.invoke(all_msgs)]}\n",
    "```\n",
    "\n",
    "**Purpose**: This function processes the conversation state and generates LLM responses.\n",
    "\n",
    "**How it works**:\n",
    "1. **System Message**: Adds a system prompt instructing the LLM to respond in plain text only (no markdown/formatting)\n",
    "2. **Message Preparation**: Combines the system message with existing conversation messages\n",
    "3. **LLM Invocation**: Calls the Databricks LLM with the complete message history\n",
    "4. **State Update**: Returns the new message to be added to the conversation state\n",
    "\n",
    "## üï∏Ô∏è **Graph Construction**\n",
    "\n",
    "```python\n",
    "builder = StateGraph(MessagesState)  # Create graph with message state management\n",
    "builder.add_node(\"call_llm\", call_llm)  # Add the LLM processing node\n",
    "builder.add_edge(START, \"call_llm\")  # Connect start to LLM node\n",
    "builder.add_edge(\"call_llm\", END)  # Connect LLM node to end\n",
    "graph = builder.compile()  # Compile the graph for execution\n",
    "```\n",
    "\n",
    "**Graph Flow**: `START ‚Üí call_llm ‚Üí END`\n",
    "\n",
    "This creates a simple linear workflow where:\n",
    "- The conversation starts\n",
    "- Messages are processed by the LLM\n",
    "- The conversation ends\n",
    "\n",
    "## üöÄ **Execution**\n",
    "\n",
    "```python\n",
    "messages = graph.invoke({\"messages\": [HumanMessage(\"Tell me more about Databricks ?\")]})\n",
    "```\n",
    "\n",
    "**What happens**:\n",
    "1. **Input**: Creates a human message asking about Databricks\n",
    "2. **Processing**: The graph processes this through the `call_llm` node\n",
    "3. **Output**: Returns the complete conversation including the LLM's response\n",
    "4. **Result**: The `messages` variable contains both the input question and the generated answer\n",
    "\n",
    "## üéØ **Key Concepts**\n",
    "\n",
    "- **StateGraph**: Manages conversation state and message flow\n",
    "- **MessagesState**: Built-in state type for handling conversation messages\n",
    "- **Node**: A processing unit in the graph (here, the LLM call)\n",
    "- **Edges**: Define the flow between nodes\n",
    "- **Invoke**: Executes the graph with given input\n",
    "\n",
    "## üí° **Use Cases**\n",
    "\n",
    "This pattern is useful for:\n",
    "- Building conversational AI applications\n",
    "- Creating chatbots with Databricks LLMs\n",
    "- Implementing structured conversation flows\n",
    "- Integrating with larger AI workflows\n",
    "\n",
    "The code represents a foundational building block that can be extended with additional nodes for more complex behaviors like tool calling, memory management, or multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03cb8ac-5eda-46a7-baf5-372791fe29a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random\n",
    "from typing import Literal, TypedDict\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# import mlflow\n",
    "# mlflow.langchain.autolog()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatDatabricks(endpoint = \"databricks-gpt-oss-120b\")\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "\n",
    "    system_msg = {\"role\": \"system\", \"content\": \"Reply only with plain text. No formatting.\"}\n",
    "    all_msgs = [system_msg] + state[\"messages\"]\n",
    "\n",
    "    return {\"messages\": [llm.invoke(all_msgs)]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_llm\", call_llm)\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"call_llm\")\n",
    "builder.add_edge(\"call_llm\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "messages = graph.invoke({\"messages\": [HumanMessage(\"Tell me more about Databricks ?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60954157-2921-4fc2-b4a4-a79d5424412e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message = messages['messages'][-1]\n",
    "\n",
    "for part in message.content:\n",
    "    if part.get(\"type\") == \"text\":\n",
    "       ai_message = part.get(\"text\", \"\")\n",
    "\n",
    "print(ai_message)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/real.anirvan@gmail.com/build-ai-agent-that-works/requirements.txt"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.2-call-chat-models-using-langraph",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
