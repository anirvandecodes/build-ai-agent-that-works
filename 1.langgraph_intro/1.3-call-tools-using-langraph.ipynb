{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f588f-5ad5-4c14-8282-9b6435afa1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LangGraph Tool Calling - Simple Explanation\n",
    "\n",
    "This code builds a **conversational AI system** that can use **tools/functions** to answer questions, using LangGraph + Databricks LLM.\n",
    "\n",
    "\n",
    "## 🛠️ **Tool Definition**\n",
    "```python\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "```\n",
    "A simple function that multiplies two numbers. The LLM can call this when users ask math questions.\n",
    "\n",
    "## 🤖 **LLM Setup**\n",
    "```python\n",
    "llm = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\")\n",
    "llm_with_tools = llm.bind_tools([multiply])  # Give LLM access to the multiply tool\n",
    "```\n",
    "\n",
    "## 🕸️ **Graph Structure**\n",
    "\n",
    "\n",
    "START → tool_calling_llm → [conditional] → tools → tool_calling_llm → END\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **tool_calling_llm**: LLM decides whether to use tools or respond directly\n",
    "- **tools**: Executes the actual tool (multiply function)\n",
    "- **Conditional edge**: Routes to tools only if LLM wants to use them\n",
    "\n",
    "## 🔄 **How It Works**\n",
    "1. User asks: *\"What is the weather in Tokyo? and what is 2 * 3?\"*\n",
    "2. LLM analyzes the question\n",
    "3. For \"2 * 3\", it calls the `multiply(2, 3)` tool\n",
    "4. Gets result `6` from the tool\n",
    "5. Responds with both weather info and math result\n",
    "\n",
    "## 🎯 **Key Features**\n",
    "- **Smart routing**: LLM automatically decides when to use tools\n",
    "- **Tool integration**: Functions become available to the AI\n",
    "- **Conversation flow**: Maintains chat history and context\n",
    "- **Conditional logic**: Uses tools only when needed\n",
    "\n",
    "The output shows each message in the conversation, including tool calls and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c796a5-e868-40fc-827c-9f0ff0e3226d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random\n",
    "from typing import Literal, TypedDict\n",
    "from langchain_core.messages import AnyMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "import mlflow\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "class State(MessagesState):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    \"\"\"Example:\n",
    "    >>> multiply(2, 3)\n",
    "    6\n",
    "    \"\"\"\n",
    "    \"\"\"Args:\n",
    "    a: int\n",
    "    b: int\n",
    "    \"\"\"\n",
    "    \"\"\"Returns:\n",
    "    int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "llm = ChatDatabricks(endpoint = \"databricks-gpt-oss-120b\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "def tool_calling_llm(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = graph.invoke({\"messages\": [HumanMessage(\"Hello,what is the weather in Tokyo? and what is 2 * 3?\")]})\n",
    "\n",
    "\n",
    "for i, message in enumerate(result[\"messages\"]):\n",
    "    print(f\"Message {i+1} ({type(message).__name__}): {message.content}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03cb8ac-5eda-46a7-baf5-372791fe29a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random\n",
    "from typing import Literal, TypedDict\n",
    "from langchain_core.messages import AnyMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "import mlflow\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "class State(MessagesState):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    \"\"\"Example:\n",
    "    >>> multiply(2, 3)\n",
    "    6\n",
    "    \"\"\"\n",
    "    \"\"\"Args:\n",
    "    a: int\n",
    "    b: int\n",
    "    \"\"\"\n",
    "    \"\"\"Returns:\n",
    "    int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "llm = ChatDatabricks(endpoint = \"databricks-gpt-oss-120b\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "def tool_calling_llm(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode([multiply]))\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\"tool_calling_llm\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"tool_calling_llm\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = graph.invoke({\"messages\": [HumanMessage(\"Hello,what is the weather in Tokyo? and what is 2 * 3?\")]})\n",
    "\n",
    "\n",
    "for i, message in enumerate(result[\"messages\"]):\n",
    "    print(f\"Message {i+1} ({type(message).__name__}): {message.content}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef08df52-8974-4759-8805-f6c6a9e96f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "LangChain Inbuilt tools https://python.langchain.com/docs/integrations/tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945c9e74-5899-46cf-bb18-90840335a53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a4df8f-67d3-46d3-a3f4-ffa52c34a354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random\n",
    "from typing import Literal, TypedDict\n",
    "from langchain_core.messages import AnyMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "import mlflow\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "class State(MessagesState):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatDatabricks(endpoint = \"databricks-gpt-oss-120b\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([wikipedia])\n",
    "\n",
    "def tool_calling_llm(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode([wikipedia]))\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\"tool_calling_llm\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"tool_calling_llm\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = graph.invoke({\"messages\": [HumanMessage(\"Tell me about databricks.\")]})\n",
    "\n",
    "\n",
    "for i, message in enumerate(result[\"messages\"]):\n",
    "    print(f\"Message {i+1} ({type(message).__name__}): {message.content}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/real.anirvan@gmail.com/build-ai-agent-that-works/requirements.txt"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.3-call-tools-using-langraph",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
