{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb12ca16-abbe-4597-a69d-a0fa95b2afac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Literal, TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\")\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "    \"\"\"Node to call the LLM with a formatted prompt based on the latest user question.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt_template = (\n",
    "        \"You are an expert AI assistant. Answer the user's question with clarity, accuracy, and conciseness.\\n\\n\"\n",
    "        \"## Question:\\n\"\n",
    "        \"{question}\\n\\n\"\n",
    "        \"## Guidelines:\\n\"\n",
    "        \"- Keep responses factual and to the point.\\n\"\n",
    "        \"- If relevant, provide examples or step-by-step instructions.\\n\"\n",
    "        \"- If the question is ambiguous, clarify before answering.\\n\\n\"\n",
    "        \"Respond below:\"\n",
    "    )\n",
    "    formatted_prompt = prompt_template.format(question=state['messages'])\n",
    "\n",
    "    # Construct the message list for the model\n",
    "    all_msgs = [\n",
    "        SystemMessage(content=\"You are an expert AI assistant.\"),\n",
    "        HumanMessage(content=formatted_prompt)\n",
    "    ]\n",
    "\n",
    "    # Call the LLM\n",
    "    response = llm.invoke(all_msgs)\n",
    "\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_llm\", call_llm)\n",
    "\n",
    "builder.add_edge(START, \"call_llm\")\n",
    "builder.add_edge(\"call_llm\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Example run\n",
    "messages = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is the return policy for Anirvan Decodes ecommerce products?\")]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc628c1f-fb86-480c-a923-04affb8ba5dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message = messages['messages'][-1]\n",
    "\n",
    "for part in message.content:\n",
    "    if part.get(\"type\") == \"text\":\n",
    "       ai_message = part.get(\"text\", \"\")\n",
    "\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f28824-e9dc-49e7-b1a5-cfc1bd0787c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## This is where we need RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12435257-eb98-4907-a824-717075c65ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Why We Need RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "### üß† LLMs don‚Äôt know everything\n",
    "- They‚Äôre trained on data up to a certain time.  \n",
    "- They can‚Äôt magically know your company policies, recent data, or private documents.\n",
    "\n",
    "### ü§î LLMs can ‚Äúhallucinate‚Äù\n",
    "- Sometimes they just make stuff up ‚Äî confidently!  \n",
    "- Without grounding in real data, they may generate wrong or outdated answers.\n",
    "\n",
    "### üîÑ Knowledge changes\n",
    "- Laws, prices, product specs ‚Äî they all change.  \n",
    "- You don‚Äôt want to retrain or fine-tune a giant model every time something updates.\n",
    "\n",
    "### üè¢ You need domain-specific answers\n",
    "- Example: A bank chatbot answering questions about your bank‚Äôs loans ‚Äî it needs **your** documents, not just generic financial knowledge.\n",
    "\n",
    "### ‚ö° Cheaper & faster than fine-tuning\n",
    "- Instead of retraining an entire model, you just fetch relevant info (retrieval) and pass it along with the question.  \n",
    "- That way, the model uses the right context instantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b3387a-6fb0-4e1d-ad29-48a68b12cf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ†Ô∏è How to Build a RAG System (Simple Terms)\n",
    "\n",
    "### 1. Collect & prepare your knowledge\n",
    "- Gather the stuff you want the AI to know: PDFs, web pages, database rows, manuals, FAQs, etc.  \n",
    "- Break big documents into small, meaningful chunks (like splitting a book into paragraphs).\n",
    "\n",
    "### 2. Index the knowledge so it‚Äôs searchable\n",
    "- Turn those chunks into numbers (vectors) so a computer can ‚Äúsearch by meaning‚Äù instead of just keywords.  \n",
    "- Store them in a **vector database** (think: a smart search engine that understands concepts, not just exact words).\n",
    "\n",
    "### 3. Connect retrieval to generation\n",
    "- When someone asks a question:  \n",
    "  - First, **search** the vector database for the most relevant chunks.  \n",
    "  - Then, **feed** those chunks + the question into your LLM (like GPT).  \n",
    "  - The LLM reads the context and gives a **grounded** answer (based on the actual data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcc0029-da8f-4d5f-9def-439495b10741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Intuition behind Embedding search\n",
    "\n",
    "## What‚Äôs the problem with plain keyword search?\n",
    "- Computers match **exact words**, not **meaning**.\n",
    "- If a user types: ‚Äú**laptop battery problem**‚Äù\n",
    "  - Keyword search may **miss** a doc that says: ‚Äú**notebook isn‚Äôt charging**‚Äù\n",
    "  - Different words, **same idea** ‚Üí missed!\n",
    "\n",
    "## What‚Äôs an embedding (in plain words)?\n",
    "An **embedding** turns text into a list of numbers (a vector) that captures **meaning**.\n",
    "- Items with **similar meaning** ‚Üí vectors are **close** together.\n",
    "- Items with **different meaning** ‚Üí vectors are **far** apart.\n",
    "\n",
    "Think of a giant map:\n",
    "- ‚Äúlaptop‚Äù and ‚Äúnotebook‚Äù are neighbors.\n",
    "- ‚Äúbanana‚Äù is far away from both.\n",
    "\n",
    "## Why embeddings make search better\n",
    "- **Synonyms & paraphrases:** ‚Äúnot charging‚Äù ‚âà ‚Äúbattery problem‚Äù\n",
    "- **Context awareness:** ‚ÄúApple‚Äù (fruit) vs. ‚ÄúApple‚Äù (company)\n",
    "- **Cross-language:** ‚Äúordenador‚Äù (ES) ‚âà ‚Äúcomputer‚Äù (EN)\n",
    "- **Typos & phrasing differences:** ‚Äúbattry problm‚Äù still finds ‚Äúbattery problem‚Äù\n",
    "\n",
    "## How embedding-based search works (step-by-step)\n",
    "1. **Prepare your data**\n",
    "   - Split docs into **small chunks** (e.g., paragraphs).\n",
    "2. **Embed the chunks**\n",
    "   - Convert each chunk into a **vector** (embedding).\n",
    "   - Store vectors in a **vector database** (FAISS, Chroma, Milvus, etc.).\n",
    "3. **Handle a user query**\n",
    "   - Convert the user‚Äôs query into a **vector**.\n",
    "4. **Find similar chunks**\n",
    "   - Use **vector similarity** (e.g., cosine similarity) to find the **closest** vectors.\n",
    "5. **Return ranked results**\n",
    "   - Show the top-K most similar chunks (or feed them to an LLM for a grounded answer).\n",
    "\n",
    "\n",
    "\n",
    "## Where embeddings shine\n",
    "- **Customer support:** ‚Äúrefund taking too long‚Äù ‚âà ‚Äúdelay in reimbursement‚Äù\n",
    "- **Internal search:** Finds policies/procedures regardless of exact phrasing\n",
    "- **RAG systems:** Fetches the **right** context for the LLM to answer reliably\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/real.anirvan@gmail.com/ai-agents-masterclass/requirements.txt"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.1-Why knowledge assistant",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
