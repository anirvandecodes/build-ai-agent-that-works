{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409a635d-7d1c-4fff-a810-c4531adec4b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langgraph-supervisor wikipedia langchain_community databricks_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182c0aca-6304-44da-88ce-0ee3108b974e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "web_search = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8960aa2f-6ba6-4480-bca6-0c8a36e02c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Research agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e8117c-90da-4824-b7c1-65994335b266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks\n",
    ")\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-gpt-oss-120b\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[web_search],\n",
    "    prompt=(\n",
    "        \"You are a research agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with research-related tasks, DO NOT do any math\\n\"\n",
    "        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n",
    "    ),\n",
    "    name=\"research_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bee5702-952c-448e-89a0-03e716de5f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add(a: float, b: float):\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: float, b: float):\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "math_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[add, multiply, divide],\n",
    "    prompt=(\n",
    "        \"You are a math agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with math-related tasks\\n\"\n",
    "        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n",
    "    ),\n",
    "    name=\"math_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeac1b02-ab77-4dfa-9ad6-42659128e077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model=llm,\n",
    "    agents=[research_agent, math_agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing two agents:\\n\"\n",
    "        \"- a research agent. Assign research-related tasks to this agent\\n\"\n",
    "        \"- a math agent. Assign math-related tasks to this agent\\n\"\n",
    "        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "        \"Do not do any work yourself.\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a26b6f4-b390-466f-8d05-f98d6d88a153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(supervisor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e305124-aa99-4bba-a5d6-d605efbde296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for chunk in supervisor.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"find US and New York state GDP in 2024. what % of US GDP was New York state?\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c80899b-d321-433f-ae89-03bab8a28f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# code is taken from https://docs.databricks.com/aws/en/notebooks/source/generative-ai/responses-agent-langgraph.html\n",
    "\n",
    "import json\n",
    "from typing import Any, Generator\n",
    "from uuid import uuid4\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "\n",
    "############################################\n",
    "# Bridge to MLflow ResponsesAgent\n",
    "############################################\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def _responses_to_cc(self, message: dict[str, Any]) -> list[dict[str, Any]]:\n",
    "        # Simplified: just handle normal messages and tool calls\n",
    "        msg_type = message.get(\"type\")\n",
    "        if msg_type == \"function_call\":\n",
    "            return [\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"tool call\",\n",
    "                    \"tool_calls\": [\n",
    "                        {\n",
    "                            \"id\": message[\"call_id\"],\n",
    "                            \"type\": \"function\",\n",
    "                            \"function\": {\n",
    "                                \"arguments\": message[\"arguments\"],\n",
    "                                \"name\": message[\"name\"],\n",
    "                            },\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        elif msg_type == \"function_call_output\":\n",
    "            return [\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": message[\"output\"],\n",
    "                    \"tool_call_id\": message[\"call_id\"],\n",
    "                }\n",
    "            ]\n",
    "        elif msg_type == \"message\" and isinstance(message[\"content\"], list):\n",
    "            return [{\"role\": message[\"role\"], \"content\": c[\"text\"]} for c in message[\"content\"]]\n",
    "        return [{\"role\": message.get(\"role\", \"assistant\"), \"content\": message.get(\"content\", \"\")}]\n",
    "\n",
    "    def _langchain_to_responses(self, messages) -> list[dict[str, Any]]:\n",
    "        outputs = []\n",
    "        for message in messages:\n",
    "            message = message.model_dump()\n",
    "            if message[\"type\"] == \"ai\":\n",
    "                if tool_calls := message.get(\"tool_calls\"):\n",
    "                    for tc in tool_calls:\n",
    "                        outputs.append(\n",
    "                            self.create_function_call_item(\n",
    "                                id=message.get(\"id\") or str(uuid4()),\n",
    "                                call_id=tc[\"id\"],\n",
    "                                name=tc[\"name\"],\n",
    "                                arguments=json.dumps(tc[\"args\"]),\n",
    "                            )\n",
    "                        )\n",
    "                else:\n",
    "                    outputs.append(\n",
    "                        self.create_text_output_item(\n",
    "                            text=message[\"content\"],\n",
    "                            id=message.get(\"id\") or str(uuid4()),\n",
    "                        )\n",
    "                    )\n",
    "            elif message[\"type\"] == \"tool\":\n",
    "                outputs.append(\n",
    "                    self.create_function_call_output_item(\n",
    "                        call_id=message[\"tool_call_id\"],\n",
    "                        output=message[\"content\"],\n",
    "                    )\n",
    "                )\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = []\n",
    "        for msg in request.input:\n",
    "            cc_msgs.extend(self._responses_to_cc(msg.model_dump()))\n",
    "\n",
    "        for event in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                for node_data in event[1].values():\n",
    "                    for item in self._langchain_to_responses(node_data[\"messages\"]):\n",
    "                        yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=item)\n",
    "            elif event[0] == \"messages\":\n",
    "                chunk = event[1][0]\n",
    "                if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                    yield ResponsesAgentStreamEvent(\n",
    "                        **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d218b0c4-59ea-4574-a9c2-f20c4a93e79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Register the agent with MLflow\n",
    "############################################\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphResponsesAgent(supervisor)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d92f5f-1b14-4bab-b97b-c323368c2efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"find US and New York state GDP in 2024. what % of US GDP was New York state?\"}]})\n",
    "print(result.model_dump(exclude_none=True))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Multi-agent supervisor with LangGraph create_supervisor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
